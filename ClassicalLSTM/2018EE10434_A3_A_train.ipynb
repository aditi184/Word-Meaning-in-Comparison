{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2018EE10434_A3_A_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWjwfx8T5OAJ"
      },
      "source": [
        "Make a copy of this notebook and rename using your USERID in the following format, 2017CSZ8058\n",
        "\n",
        "Give read access to keshavkolluru@gmail.com, vishalsaley114@gmail.com and kartikeya.badola@gmail.com\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spAvH1fF0Rhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b45f3b-9e58-4cd2-d1af-e77e34b50cdf"
      },
      "source": [
        "## DONT CHANGE THIS CELL\n",
        "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-11 13:44:30--  http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
            "Resolving www.cse.iitd.ac.in (www.cse.iitd.ac.in)... 103.27.9.152\n",
            "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip [following]\n",
            "--2021-10-11 13:44:30--  https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
            "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 217313 (212K) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>] 212.22K  86.2KB/s    in 2.5s    \n",
            "\n",
            "2021-10-11 13:44:34 (86.2 KB/s) - ‘data.zip’ saved [217313/217313]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOE6QX5rVBix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d91862-6063-4a33-d144-f09d269b4848"
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/train/\n",
            "  inflating: data/train/train.gold.txt  \n",
            "  inflating: data/train/train.data.txt  \n",
            "   creating: data/validation/\n",
            "  inflating: data/validation/validation.data.txt  \n",
            "  inflating: data/validation/validation.gold.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwf7VQjTVczk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c28a25a-9f52-4ef4-d9d6-5548169b5770"
      },
      "source": [
        "!pip install -U torchtext==0.10.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrnkLN2LzlDB"
      },
      "source": [
        "## Import relevant packages\n",
        "\n",
        "import os\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "import torch.optim as O\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import logging\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from pdb import set_trace\n",
        "import pandas as pd\n",
        "import torchtext\n",
        "import dill\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, LabelField\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import spacy\n",
        "\n",
        "torch.manual_seed(4)\n",
        "random.seed(4)\n",
        "np.random.seed(4)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnPUKqO1xV_z"
      },
      "source": [
        "model_dir = './MODEL'\n",
        "if os.path.exists(model_dir) == False:\n",
        "  os.mkdir(model_dir)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr3ok6g51O_d"
      },
      "source": [
        "def get_device(gpu_no):\n",
        "\tif torch.cuda.is_available():\n",
        "\t\ttorch.cuda.set_device(gpu_no)\n",
        "\t\treturn torch.device('cuda:{}'.format(gpu_no))\n",
        "\telse:\n",
        "\t\treturn torch.device('cpu')\t\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V88eBu6QkU_1"
      },
      "source": [
        "def prepareDataset():\n",
        "    x_train = pd.read_csv('/content/data/train/train.data.txt', sep = '\\t',header = None, names = [\"word\",\"pos\",\"P1P2\" , \"sentence1\", \"sentence2\"])\n",
        "    y_train = pd.read_csv('/content/data/train/train.gold.txt', header = None, names = ['labels'])\n",
        "    y_train = y_train.labels.apply(lambda x: 0 if x == 'F' else 1)\n",
        "    a = x_train[\"P1P2\"].str.split(pat = \"-\",expand = True)\n",
        "    a.columns = ['P1','P2']\n",
        "    x_train = x_train.join(a)\n",
        "    x_train = x_train.join(y_train)\n",
        "    x_train.to_csv('TrainData.csv')\n",
        "\n",
        "    x_dev = pd.read_csv('/content/data/validation/validation.data.txt', sep = '\\t',header = None, names = [\"word\",\"pos\",\"P1P2\" , \"sentence1\", \"sentence2\"])\n",
        "    y_dev= pd.read_csv('/content/data/validation/validation.gold.txt', header = None, names = ['labels'])\n",
        "    y_dev = y_dev.labels.apply(lambda x: 0 if x == 'F' else 1)\n",
        "    b = x_dev[\"P1P2\"].str.split(pat = \"-\",expand = True)\n",
        "    b.columns = ['P1','P2']\n",
        "    x_dev = x_dev.join(b)\n",
        "    x_dev = x_dev.join(y_dev)\n",
        "    x_dev.to_csv('ValidationData.csv')\n",
        "    \n",
        "prepareDataset()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znEn_gzJh2S2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fe5c2b-6da4-4182-cab7-084414645dda"
      },
      "source": [
        "spacy_tokens = spacy.load('en')\n",
        "\n",
        "def tokenizer(sentence):\n",
        "    return [token.text for token in spacy_tokens.tokenizer(sentence)]\n",
        "\n",
        "sentence = Field(sequential=True, use_vocab=True,  tokenize = tokenizer , lower = True)\n",
        "position = Field(sequential=False,use_vocab=False)\n",
        "labels = LabelField(sequential=False, use_vocab=False)\n",
        "\n",
        "fields = {'sentence1' : ('sentence1' , sentence), 'sentence2' : ('sentence2' , sentence), 'word' : ('word' , sentence) , 'P1' : ('P1',position), 'P2' : ('P2',position), 'labels' : ('labels' , labels) }\n",
        "\n",
        "\n",
        "train_data , dev_data = TabularDataset.splits(path = './' , train = 'TrainData.csv' , validation='ValidationData.csv' , format = 'csv' , fields= fields )\n",
        "sentence.build_vocab(train_data, min_freq = 2,vectors = 'glove.6B.300d' )\n",
        "\n",
        "train_iter , dev_iter = BucketIterator.splits((train_data,dev_data), batch_size=32 , device = 'cpu' , sort=False,repeat = False)\n",
        "with open(os.path.join('MODEL','vocab'),'wb') as file:\n",
        "  dill.dump(sentence,file)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:45<00:00, 8857.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXtRaV5J9rQ6"
      },
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, embed_dim, hidden, n_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden = hidden\n",
        "        self.n_layers = n_layers\n",
        "        self.embed = nn.Embedding.from_pretrained(sentence.vocab.vectors, freeze=False)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden,num_layers=n_layers,bidirectional=False)#batch_first = True #dropout=0.2\n",
        "        \n",
        "  def forward(self, input):\n",
        "  \n",
        "        input.sentence1 = input.sentence1.to(device)\n",
        "        input.sentence2 = input.sentence2.to(device)\n",
        "        input.P1 = input.P1.to(device)\n",
        "        input.P2 = input.P2.to(device)\n",
        "        embed_out = self.embed(input.sentence1).to(device)\n",
        "        # embed_out = self.dropout(embed_out).to(device)\n",
        "        hidden_in = torch.zeros(self.n_layers ,input.sentence1.shape[1], self.hidden).to(device)\n",
        "        cell_in = torch.zeros( self.n_layers , input.sentence1.shape[1], self.hidden).to(device)\n",
        "        out_lstm, (hidden_out, cell_out) = self.lstm(embed_out,(hidden_in, cell_in))\n",
        "        \n",
        "        \n",
        "        \n",
        "        embed_out2 = self.embed(input.sentence2).to(device)\n",
        "        # embed_out2 = self.dropout(embed_out2).to(device)\n",
        "        hidden_in2 = torch.zeros(self.n_layers  ,input.sentence2.shape[1], self.hidden).to(device)\n",
        "        cell_in2 = torch.zeros( self.n_layers , input.sentence2.shape[1], self.hidden).to(device)\n",
        "        out_lstm2, (hidden_out2, cell_out2) = self.lstm(embed_out2,(hidden_in2, cell_in2))\n",
        "\n",
        "        \n",
        "        \n",
        "        dim_1 = out_lstm[input.P1,torch.arange(out_lstm.size(1))]\n",
        "        dim_2 = out_lstm2[input.P2,torch.arange(out_lstm2.size(1))]\n",
        "        \n",
        "        cos = nn.CosineSimilarity(dim=1, eps=1e-8)\n",
        "        output = cos(dim_1, dim_2)\n",
        "        \n",
        "        output = torch.sigmoid(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14EjR4tmz2x5"
      },
      "source": [
        "## Basic training loop\n",
        "\n",
        "class Train():\n",
        "\tdef __init__(self):\n",
        "\t\tself.lr = 0.0001\n",
        "\t\tself.epochs = 20\n",
        "\t\tself.gpu = 0\n",
        "\t\tself.device = get_device(self.gpu)\n",
        "\t\tself.step_size = 5\n",
        "\t\tself.gamma = 0.6\n",
        "\t\tself.vocab_size = len(sentence.vocab)\n",
        "\t\tself.emb_dim = sentence.vocab.vectors.shape[1]\n",
        "\t\tself.hidden = 512\n",
        "\t\tself.layers = 2\n",
        "\t\t\n",
        "\t\tself.train_iter = train_iter\n",
        "\t\tself.dev_iter = dev_iter\n",
        "\t\t\n",
        "\t\tself.model = LSTMModel(self.emb_dim,self.hidden,self.layers)\n",
        "\t\t\n",
        "\t\tself.model.to(self.device)\n",
        "\t\tself.criterion = nn.BCELoss()\n",
        "\t\tself.opt = O.Adam(self.model.parameters(), lr = self.lr)\n",
        "\t\tself.best_val_acc = None\n",
        "\t\tself.scheduler = StepLR(self.opt, step_size=self.step_size, gamma=self.gamma)\n",
        "\n",
        "\t\t\n",
        "\n",
        "\tdef result_checkpoint(self, epoch, train_loss, val_loss, train_acc, val_acc, took):\n",
        "\t\tif self.best_val_acc is None or val_acc > self.best_val_acc:\n",
        "\t\t\tself.best_val_acc = val_acc\n",
        "\t\t\ttorch.save({\n",
        "\t\t\t\t'accuracy': self.best_val_acc,\n",
        "\t\t\t\t'model_dict': self.model.state_dict(),\n",
        "\t\t\t}, './MODEL/Model.pt')\n",
        "\t\n",
        "\tdef train(self):\n",
        "\t\tself.model.train(); self.train_iter.init_epoch()\n",
        "\t\tn_correct, n_total, n_loss = 0, 0, 0\n",
        "\t\t\n",
        "\t\tfor batch_idx, batch in enumerate(train_iter):\n",
        "\t\t\tself.opt.zero_grad()\n",
        "\t\t\tanswer = self.model(batch)\n",
        "\t\t\tloss = self.criterion(answer, batch.labels.float().to(device))\n",
        "\t\t\tn_correct += ((answer>=0.5).int() == batch.labels.to(device)).float().sum()\n",
        "\t\t\tn_total += batch.batch_size\n",
        "\t\t\tn_loss += loss.item()\n",
        "\t\t\tloss.backward(); self.opt.step()\n",
        "\t\ttrain_loss = n_loss/n_total\n",
        "\t\ttrain_acc = 100. * n_correct/n_total\n",
        "\t\treturn train_loss, train_acc\n",
        "\n",
        "\tdef validate(self):\n",
        "\t\tself.model.eval(); self.dev_iter.init_epoch()\n",
        "\t\tn_correct, n_total, n_loss = 0, 0, 0\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tfor batch_idx, batch in enumerate(self.dev_iter):\n",
        "\t\t\t\tanswer = self.model(batch)\n",
        "\t\t\t\tloss = self.criterion(answer, batch.labels.float().to(device))\n",
        "\t\t\t\tn_correct += ((answer>=0.5).int() == batch.labels.to(device)).float().sum()\n",
        "\t\t\t\tn_total += batch.batch_size\n",
        "\t\t\t\tn_loss += loss.item()\n",
        "\n",
        "\t\t\tval_loss = n_loss/n_total\n",
        "\t\t\tval_acc = 100. * n_correct/n_total\n",
        "\t\t\treturn val_loss, val_acc\n",
        "\n",
        "\tdef execute(self):\n",
        "\t\tprint(\" [*] Training starts!\")\n",
        "\t\tprint('-' * 99)\n",
        "\t\tfor epoch in range(1, self.epochs+1):\n",
        "\t\t\tstart = time.time()\n",
        "\n",
        "\t\t\ttrain_loss, train_acc = self.train()\n",
        "\t\t\tval_loss, val_acc = self.validate()\n",
        "\t\t\tself.scheduler.step()\n",
        "\t\t\t\n",
        "\t\t\ttook = time.time()-start\n",
        "\t\t\tself.result_checkpoint(epoch, train_loss, val_loss, train_acc, val_acc, took)\n",
        "\n",
        "\t\t\tprint('| Epoch {:3d} | train loss {:5.5f} | train acc {:5.5f} | val loss {:5.5f} | val acc {:5.5f} | time: {:5.2f}s |'.format(\n",
        "\t\t\t\tepoch, train_loss, train_acc, val_loss, val_acc, took))\n",
        "\t\tself.finish()\n",
        "\n",
        "\tdef finish(self):\n",
        "\t\tprint(\"[*] Training finished!\\n\\n\")\n",
        "\t\tprint('-' * 99)\n",
        "\t\tprint(\" [*] Training finished!\")\n",
        "\t\tprint(\" [*] Please find the saved model and training log in results_dir\")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM2QDq2iz-h7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25fc70d9-d35b-4518-abb3-b4f8aea3bf07"
      },
      "source": [
        "task = Train()\n",
        "task.execute()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [*] Training starts!\n",
            "---------------------------------------------------------------------------------------------------\n",
            "| Epoch   1 | train loss 0.02200 | train acc 53.37141 | val loss 0.02140 | val acc 56.26959 | time: 144.05s |\n",
            "| Epoch   2 | train loss 0.02080 | train acc 61.82756 | val loss 0.02103 | val acc 58.93417 | time: 141.15s |\n",
            "| Epoch   3 | train loss 0.02018 | train acc 64.90420 | val loss 0.02113 | val acc 60.03135 | time: 140.95s |\n",
            "| Epoch   4 | train loss 0.01966 | train acc 68.23876 | val loss 0.02082 | val acc 61.12852 | time: 142.49s |\n",
            "| Epoch   5 | train loss 0.01916 | train acc 70.76271 | val loss 0.02088 | val acc 62.22570 | time: 140.33s |\n",
            "| Epoch   6 | train loss 0.01861 | train acc 73.78408 | val loss 0.02085 | val acc 61.12852 | time: 141.51s |\n",
            "| Epoch   7 | train loss 0.01832 | train acc 75.64481 | val loss 0.02091 | val acc 60.50157 | time: 141.44s |\n",
            "| Epoch   8 | train loss 0.01805 | train acc 76.25276 | val loss 0.02091 | val acc 60.34483 | time: 140.41s |\n",
            "| Epoch   9 | train loss 0.01776 | train acc 77.54237 | val loss 0.02081 | val acc 64.42006 | time: 140.41s |\n",
            "| Epoch  10 | train loss 0.01755 | train acc 78.15033 | val loss 0.02095 | val acc 62.22570 | time: 139.68s |\n",
            "| Epoch  11 | train loss 0.01718 | train acc 80.06632 | val loss 0.02096 | val acc 61.44201 | time: 140.16s |\n",
            "| Epoch  12 | train loss 0.01701 | train acc 80.60059 | val loss 0.02100 | val acc 60.65831 | time: 139.67s |\n",
            "| Epoch  13 | train loss 0.01685 | train acc 81.19012 | val loss 0.02106 | val acc 60.34483 | time: 139.39s |\n",
            "| Epoch  14 | train loss 0.01667 | train acc 81.77966 | val loss 0.02109 | val acc 61.12852 | time: 139.50s |\n",
            "| Epoch  15 | train loss 0.01656 | train acc 82.24023 | val loss 0.02114 | val acc 60.03135 | time: 141.05s |\n",
            "| Epoch  16 | train loss 0.01636 | train acc 82.64554 | val loss 0.02116 | val acc 60.03135 | time: 139.52s |\n",
            "| Epoch  17 | train loss 0.01626 | train acc 83.27193 | val loss 0.02119 | val acc 59.56113 | time: 139.26s |\n",
            "| Epoch  18 | train loss 0.01618 | train acc 83.41930 | val loss 0.02120 | val acc 59.87461 | time: 140.90s |\n",
            "| Epoch  19 | train loss 0.01609 | train acc 83.65881 | val loss 0.02127 | val acc 59.24765 | time: 141.96s |\n",
            "| Epoch  20 | train loss 0.01602 | train acc 83.69566 | val loss 0.02126 | val acc 59.71787 | time: 141.05s |\n",
            "[*] Training finished!\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            " [*] Training finished!\n",
            " [*] Please find the saved model and training log in results_dir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-j7z1e6OjGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7fefae-b98c-44ec-e129-3682db8a6b4e"
      },
      "source": [
        "## Zip the final model and all the required files, such as vocabulary\n",
        "# Replace USERID with your own, such as 2017CSZ8058\n",
        "!zip -r 2018EE10434_A_model.zip MODEL\n",
        "\n",
        "## Upload it to Google drive and ensure that the testing notebook uses the correct link"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: MODEL/ (stored 0%)\n",
            "  adding: MODEL/vocab (deflated 11%)\n",
            "  adding: MODEL/Model.pt (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VTKz6SSvpcw"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}